{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u_ULRBS9kna",
        "outputId": "9637bfc8-620f-4990-e6a6-5f6242792210"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports section\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/StackGAN/Modules')\n",
        "\n",
        "import generate_dataset as gd # Self made dataset pipelining module\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import pickle\n",
        "import cv2\n",
        "import time\n",
        "import h5py"
      ],
      "metadata": {
        "id": "oVq5JwY89pVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Miscellaneous Functions**"
      ],
      "metadata": {
        "id": "eBofUyjsdssM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_image(image, filename, img_path):\n",
        "  # Saves image at specified file path\n",
        "  if os.path.exists(img_path)==False:\n",
        "    os.makedirs(img_path)\n",
        "  cv2.imwrite((img_path+filename), image)"
      ],
      "metadata": {
        "id": "JQizoV4SdsIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kulback Liebler Loss for Generator**"
      ],
      "metadata": {
        "id": "XSkYZwbsDRAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KL Divergence is used as a regularization and latent space control parameter\n",
        "def KL_adversarial_loss(y_true, y_pred):\n",
        "  mean=y_pred[:, :128]\n",
        "  logsigma=y_pred[:, 128:]\n",
        "  return K.mean(-logsigma+0.5*(-1+K.exp(2.0*logsigma)+K.square(mean)))"
      ],
      "metadata": {
        "id": "4PtJLJfaBB-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conditional Augmentation**"
      ],
      "metadata": {
        "id": "-qg6-vhHDMBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the conditional augmentation function\n",
        "def conditional_augmentation(x):\n",
        "  mean=x[:, :128]\n",
        "  log_sigma=x[:, 128:]\n",
        "  stddev = tf.exp(log_sigma)\n",
        "  epsilon = tf.random.normal(tf.shape(mean), dtype=tf.float32)\n",
        "  c = mean + stddev * epsilon\n",
        "  return c\n",
        "\n",
        "# Build the Conditional Augmentation Network (CAN) model\n",
        "def build_ca_network(input_shape=(1024,), latent_dim=256):\n",
        "  input_layer = tf.keras.layers.Input(shape=input_shape)\n",
        "  x = tf.keras.layers.Dense(latent_dim)(input_layer)\n",
        "  x = tf.keras.layers.LeakyReLU(x)\n",
        "  ca = tf.keras.layers.Lambda(conditional_augmentation)(x)\n",
        "\n",
        "  # Compiling the model\n",
        "  model = tf.keras.models.Model(inputs=input_layer, outputs=ca)\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy')\n",
        "\n",
        "  return model\n",
        "\n",
        "# Build an embedding compressor to 128 for Char_CNN_RNN\n",
        "def embedding_compressor():\n",
        "  input=tf.keras.layers.Input(shape=(1024,))\n",
        "  compressed=tf.keras.layers.Dense(units=128)(input)\n",
        "  compressed=tf.keras.layers.LeakyReLU(alpha=0.01)(compressed)\n",
        "\n",
        "  model=tf.keras.Model(inputs=[input], outputs=[compressed])\n",
        "  return model"
      ],
      "metadata": {
        "id": "UiOQ7lkeCvCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stage 1 Generator Model**"
      ],
      "metadata": {
        "id": "13Y4__FTH-z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upsampling block for generator\n",
        "def upSample(x,dim):\n",
        "  x=tf.keras.layers.Conv2DTranspose(dim, kernel_size=3, strides=(2, 2), padding=\"same\", kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
        "  x=tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)(x)\n",
        "  x=tf.nn.relu(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "# Stage 1 Generator builder\n",
        "def Stage1_Generator():\n",
        "  # Conditional Augmentation of  embedding vector\n",
        "  inp=tf.keras.Input(shape=(1024,))\n",
        "  ca=tf.keras.layers.Dense(256)(inp)\n",
        "  ca=tf.keras.layers.LeakyReLU(alpha=0.01)(ca)\n",
        "  c=tf.keras.layers.Lambda(conditional_augmentation)(ca)\n",
        "\n",
        "  # Latent space conditioning\n",
        "  noise=tf.keras.Input(shape=(100,))\n",
        "  gen_input=tf.keras.layers.Concatenate(axis = 1)([c,noise])\n",
        "  x=tf.keras.layers.Dense(units = 128*8*4*4, kernel_initializer = tf.keras.initializers.HeUniform())(gen_input)\n",
        "  x=tf.keras.layers.LeakyReLU(alpha=0.01)(x)\n",
        "  x=tf.keras.layers.Reshape(target_shape = (4, 4, 128*8), input_shape = (128*8*4*4, ))(x)\n",
        "  x=tf.keras.layers.BatchNormalization(axis = -1, momentum = 0.99)(x)\n",
        "\n",
        "  # Upsampling and deconvoluting latent space\n",
        "  x=upSample(x,512)\n",
        "  x=upSample(x,256)\n",
        "  x=upSample(x,128)\n",
        "  x=upSample(x,64)\n",
        "  x=tf.keras.layers.Conv2DTranspose(3,kernel_size=3,strides=1,padding=\"same\",kernel_initializer=tf.keras.initializers.GlorotUniform())(x)\n",
        "  x=tf.nn.tanh(x)\n",
        "  # Model\n",
        "  stack1_gen=tf.keras.Model(inputs=[inp,noise], outputs=[x])\n",
        "  return stack1_gen"
      ],
      "metadata": {
        "id": "oG5oFvJaG9Gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator=Stage1_Generator()\n",
        "generator.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzXh3P8OSP4c",
        "outputId": "fe6d042e-09cc-4ca7-bb65-34934be4a866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 1024)]               0         []                            \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 256)                  262400    ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " leaky_re_lu (LeakyReLU)     (None, 256)                  0         ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " lambda (Lambda)             (None, 128)                  0         ['leaky_re_lu[0][0]']         \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 100)]                0         []                            \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 228)                  0         ['lambda[0][0]',              \n",
            "                                                                     'input_2[0][0]']             \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 16384)                3751936   ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 16384)                0         ['dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " reshape (Reshape)           (None, 4, 4, 1024)           0         ['leaky_re_lu_1[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization (Batch  (None, 4, 4, 1024)           4096      ['reshape[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_transpose (Conv2DTr  (None, 8, 8, 512)            4719104   ['batch_normalization[0][0]'] \n",
            " anspose)                                                                                         \n",
            "                                                                                                  \n",
            " batch_normalization_1 (Bat  (None, 8, 8, 512)            2048      ['conv2d_transpose[0][0]']    \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " tf.nn.relu (TFOpLambda)     (None, 8, 8, 512)            0         ['batch_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_transpose_1 (Conv2D  (None, 16, 16, 256)          1179904   ['tf.nn.relu[0][0]']          \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_2 (Bat  (None, 16, 16, 256)          1024      ['conv2d_transpose_1[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " tf.nn.relu_1 (TFOpLambda)   (None, 16, 16, 256)          0         ['batch_normalization_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_transpose_2 (Conv2D  (None, 32, 32, 128)          295040    ['tf.nn.relu_1[0][0]']        \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_3 (Bat  (None, 32, 32, 128)          512       ['conv2d_transpose_2[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " tf.nn.relu_2 (TFOpLambda)   (None, 32, 32, 128)          0         ['batch_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_transpose_3 (Conv2D  (None, 64, 64, 64)           73792     ['tf.nn.relu_2[0][0]']        \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_4 (Bat  (None, 64, 64, 64)           256       ['conv2d_transpose_3[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " tf.nn.relu_3 (TFOpLambda)   (None, 64, 64, 64)           0         ['batch_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_transpose_4 (Conv2D  (None, 64, 64, 3)            1731      ['tf.nn.relu_3[0][0]']        \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " tf.math.tanh (TFOpLambda)   (None, 64, 64, 3)            0         ['conv2d_transpose_4[0][0]']  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 10291843 (39.26 MB)\n",
            "Trainable params: 10287875 (39.25 MB)\n",
            "Non-trainable params: 3968 (15.50 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stage 1 Discriminator**"
      ],
      "metadata": {
        "id": "LO8eSwZVMnjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Stage 1 Discriminator\n",
        "def Stage1_Discriminator():\n",
        "  # Input image\n",
        "  input1 = tf.keras.Input(shape=(64, 64, 3))\n",
        "  x = tf.keras.layers.Conv2D(64, kernel_size=(4,4), strides=2, padding='same', use_bias=False, kernel_initializer=tf.keras.initializers.HeUniform())(input1)\n",
        "  x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
        "  # Downsampling and convoluting image\n",
        "  x = downSample(x, 128)\n",
        "  x = downSample(x, 256)\n",
        "  x = downSample(x, 512)\n",
        "\n",
        "  # Concatenating compressing text embedding to generated distribution\n",
        "  input2 = tf.keras.Input(shape=(4,4,128,)) # Text embedding\n",
        "  x = tf.keras.layers.concatenate([x, input2])\n",
        "  x = tf.keras.layers.Conv2D(512, kernel_size=(1,1), padding='same', strides=1, use_bias=False,kernel_initializer='he_uniform')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "  # Logit generation\n",
        "  x= tf.keras.layers.Flatten()(x)\n",
        "  x = tf.keras.layers.Dense(1, kernel_initializer=tf.keras.initializers.GlorotUniform())(x) # 0 or 1\n",
        "  x = tf.keras.layers.Activation('tanh')(x)\n",
        "  # Model\n",
        "  stage1_dis = tf.keras.Model(inputs=[input1, input2], outputs=[x])\n",
        "  return stage1_dis\n",
        "\n",
        "# Downsampling block for Discriminator\n",
        "def downSample(x, kernels, kernel_size=(4,4), strides=2, activation=True):\n",
        "  x = tf.keras.layers.Conv2D(kernels, kernel_size=kernel_size, padding='same', strides=strides, use_bias=False, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  if activation:\n",
        "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
        "  return x"
      ],
      "metadata": {
        "id": "3TVpWYS6MrmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator=Stage1_Discriminator()\n",
        "discriminator.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuW4Llk2UHh2",
        "outputId": "1fe9cf9c-5dd6-4178-9e66-419dbf7b425a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)        [(None, 64, 64, 3)]          0         []                            \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)             (None, 32, 32, 64)           3072      ['input_3[0][0]']             \n",
            "                                                                                                  \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 32, 32, 64)           0         ['conv2d[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)           (None, 16, 16, 128)          131072    ['leaky_re_lu_2[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_5 (Bat  (None, 16, 16, 128)          512       ['conv2d_1[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 16, 16, 128)          0         ['batch_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)           (None, 8, 8, 256)            524288    ['leaky_re_lu_3[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_6 (Bat  (None, 8, 8, 256)            1024      ['conv2d_2[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 8, 8, 256)            0         ['batch_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)           (None, 4, 4, 512)            2097152   ['leaky_re_lu_4[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_7 (Bat  (None, 4, 4, 512)            2048      ['conv2d_3[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 4, 4, 512)            0         ['batch_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)        [(None, 4, 4, 128)]          0         []                            \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate  (None, 4, 4, 640)            0         ['leaky_re_lu_5[0][0]',       \n",
            " )                                                                   'input_4[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)           (None, 4, 4, 512)            327680    ['concatenate_1[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_8 (Bat  (None, 4, 4, 512)            2048      ['conv2d_4[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 4, 4, 512)            0         ['batch_normalization_8[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 8192)                 0         ['leaky_re_lu_6[0][0]']       \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 1)                    8193      ['flatten[0][0]']             \n",
            "                                                                                                  \n",
            " activation (Activation)     (None, 1)                    0         ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3097089 (11.81 MB)\n",
            "Trainable params: 3094273 (11.80 MB)\n",
            "Non-trainable params: 2816 (11.00 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stage 1 Adversarial Model**"
      ],
      "metadata": {
        "id": "S7hPj2lZM9QP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Stage 1 Adversarial model\n",
        "def Stage1_Adversarial(generator_model, discriminator_model):\n",
        "    input1 = tf.keras.Input(shape=(1024,)) # Text embedding\n",
        "    input2 = tf.keras.Input(shape=(100,)) # Noise\n",
        "    input3 = tf.keras.Input(shape=(4,4,128,)) # Compressed embedding\n",
        "\n",
        "    img = generator_model([input1, input2]) # Text, noise\n",
        "\n",
        "  # Discriminator not trainable during adversarial game\n",
        "    discriminator_model.trainable = False\n",
        "\n",
        "  # Model\n",
        "    discrimOutput = discriminator_model([img, input3])\n",
        "    adversarial_model = tf.keras.Model(inputs=[input1, input2, input3], outputs=[discrimOutput])\n",
        "    return adversarial_model"
      ],
      "metadata": {
        "id": "AiBngrx2NATv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adversarial=Stage1_Adversarial(generator, discriminator)\n",
        "adversarial.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "my93mwj_WQ9R",
        "outputId": "379e4ef4-40c1-4550-bbef-47bb20447ef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_72\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_151 (InputLayer)      [(None, 1024)]               0         []                            \n",
            "                                                                                                  \n",
            " input_152 (InputLayer)      [(None, 100)]                0         []                            \n",
            "                                                                                                  \n",
            " model (Functional)          (None, 64, 64, 3)            1029184   ['input_151[0][0]',           \n",
            "                                                          3          'input_152[0][0]']           \n",
            "                                                                                                  \n",
            " input_153 (InputLayer)      [(None, 4, 4, 128)]          0         []                            \n",
            "                                                                                                  \n",
            " model_1 (Functional)        (None, 1)                    3097089   ['model[6][0]',               \n",
            "                                                                     'input_153[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 13388932 (51.07 MB)\n",
            "Trainable params: 10287875 (39.25 MB)\n",
            "Non-trainable params: 3101057 (11.83 MB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stage 1 Model**"
      ],
      "metadata": {
        "id": "uaQaCmsHQEUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Stage1_GAN:\n",
        "  def __init__(self, train_ds, epochs=400, z_dim=100, batch_size=64, gen_lr=0.001, dis_lr=0.001):\n",
        "    self.epochs=epochs\n",
        "    self.z_dim=z_dim\n",
        "    self.batch_size=batch_size\n",
        "    self.gen_lr=gen_lr\n",
        "    self.dis_lr=dis_lr\n",
        "    self.img_size=64\n",
        "    # Training dataset\n",
        "    self.train_ds=train_ds\n",
        "\n",
        "    # Optimizers to the models\n",
        "    self.gen_optimizer=tf.keras.optimizers.Adam(beta_1=0.6, beta_2=0.999, learning_rate=self.gen_lr)\n",
        "    self.dis_optimizer=tf.keras.optimizers.Adam(beta_1=0.6, beta_2=0.999, learning_rate=self.dis_lr)\n",
        "\n",
        "    # Models\n",
        "    self.generator=Stage1_Generator()\n",
        "    self.generator.compile(optimizer=self.gen_optimizer, loss='binary_crossentropy')\n",
        "    self.discriminator=Stage1_Discriminator()\n",
        "    self.discriminator.loss_function=tf.nn.sigmoid_cross_entropy_with_logits\n",
        "    self.discriminator.compile(optimizer=self.dis_optimizer, loss=self.discriminator.loss_function)\n",
        "\n",
        "    # Embedding Compressor\n",
        "    self.embed_compressor=embedding_compressor()\n",
        "    self.embed_compressor.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy')\n",
        "\n",
        "    # Adversarial Model\n",
        "    self.model=Stage1_Adversarial(self.generator, self.discriminator)\n",
        "    self.model.compile(loss=['binary_crossentropy', KL_adversarial_loss], loss_weights=[1., 2.], optimizer=self.gen_optimizer)\n",
        "\n",
        "    self.checkpoint1 = tf.train.Checkpoint(gen_optimizer=self.gen_optimizer, discriminator_optimizer=self.dis_optimizer, generator=self.generator,discriminator=self.discriminator)\n",
        "\n",
        "  def train_GAN(self):\n",
        "    self.gen_loss_log=[]\n",
        "    self.dis_loss_log=[]\n",
        "    start=time.time()\n",
        "    for i in range(self.epochs):\n",
        "      gen_epoch_loss=[]\n",
        "      dis_epoch_loss=[]\n",
        "      print(\"<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\")\n",
        "      print(\"<><><><><><><><> Started Epoch:\",(i+1),\"<><><><><><><><><><><><><><><><><><><>\")\n",
        "\n",
        "      # Current batch from dataset\n",
        "      real_img, real_embed=next(self.train_ds)\n",
        "      if((i+1)%75==0):\n",
        "        K.set_value(self.gen_optimizer.learning_rate, self.gen_optimizer.learning_rate/2)\n",
        "        K.set_value(self.dis_optimizer.learning_rate, self.dis_optimizer.learning_rate/2)\n",
        "\n",
        "      # Iterating through mini-batches\n",
        "      num_iters=125\n",
        "      for iter in range(num_iters):\n",
        "        if((iter+1)%5==0):\n",
        "          print(\":3 \", end='')\n",
        "\n",
        "        # To train discriminator on real images-real captions\n",
        "        if(iter!=0):\n",
        "          real_img, real_embed=next(self.train_ds)\n",
        "        real_labels= tf.random.uniform(shape=(self.batch_size, 1), minval = .9, maxval = 1.)\n",
        "\n",
        "        # To train discriminator on real images-mismatched real captions\n",
        "        mismatched_img=tf.roll(real_img, shift=1, axis=0)\n",
        "        mismatched_labels= tf.random.uniform(shape=(self.batch_size, 1), minval = .9, maxval = 1.)\n",
        "\n",
        "        # To train discriminator on fake images-real captions\n",
        "        fake_img=self.generator([real_embed, tf.random.normal(shape=(self.batch_size, self.z_dim), stddev=0.2)])\n",
        "        fake_labels= tf.random.uniform(shape=(self.batch_size, 1), minval = 0., maxval = .1)\n",
        "\n",
        "        # Real captions compressed and reshaped for discriminator\n",
        "        compressed_embed=self.embed_compressor.predict_on_batch(real_embed)\n",
        "        compressed_embed=tf.reshape(compressed_embed, (-1, 1, 1, 128))\n",
        "        compressed_embed=tf.tile(compressed_embed, (1, 4, 4, 1))\n",
        "        self.discriminator.trainable=True\n",
        "        # Train Discriminator manually\n",
        "        with tf.GradientTape() as dis_tape:\n",
        "          # Discriminator logits\n",
        "          real_logits=self.discriminator(inputs=[real_img, compressed_embed])\n",
        "          fake_logits=self.discriminator(inputs=[fake_img, compressed_embed])\n",
        "          mismatched_logits=self.discriminator(inputs=[mismatched_img, compressed_embed])\n",
        "\n",
        "          # Loss for each logit\n",
        "          loss_real=tf.reduce_mean(self.discriminator.loss_function(real_labels, real_logits))\n",
        "          loss_fake=tf.reduce_mean(self.discriminator.loss_function(fake_labels, fake_logits))\n",
        "          loss_mismatched=tf.reduce_mean(self.discriminator.loss_function(mismatched_labels, mismatched_logits))\n",
        "\n",
        "          # Total discriminator weighted loss\n",
        "          dis_loss=tf.reduce_mean(0.5*tf.add(loss_real, 0.5*tf.add(loss_fake, loss_mismatched)))\n",
        "\n",
        "        # Discriminator gradient descent on optimizer\n",
        "        discriminator_gradient=dis_tape.gradient(dis_loss, self.discriminator.trainable_variables)\n",
        "        print(discriminator_gradient[-1])\n",
        "        self.dis_optimizer.apply_gradients(zip(discriminator_gradient, self.discriminator.trainable_variables))\n",
        "        dis_epoch_loss.append([\"Batch:\",(iter+1),\"|| Loss:\", dis_loss])\n",
        "        self.discriminator.trainable=False\n",
        "        # Training Adversarial GAN model\n",
        "        gen_loss=self.model.train_on_batch([real_embed, tf.random.normal(shape=(self.batch_size, self.z_dim), stddev=0.2), compressed_embed], [real_labels])\n",
        "        gen_epoch_loss.append([\"Batch:\",(iter+1),\"|| Loss:\", gen_loss])\n",
        "\n",
        "      print(\"\\nDiscriminator's Loss after epoch number\",i,\"is=\",dis_loss)\n",
        "      print(\"Generator's Loss after epoch number\",i,\"is=\",gen_loss)\n",
        "\n",
        "      # Saving 20 generator images after every 50 epochs\n",
        "      if(i%50==0):\n",
        "        gen_images=self.generator.predict_on_batch([real_embed, tf.random.normal(shape=(self.batch_size, self.z_dim), stddev=0.2)])\n",
        "        for num, image in enumerate(gen_images[:10]):\n",
        "          image=127.5*image+127.5\n",
        "          save_image(image, f'{i+1}_{num}.jpg', '/content/drive/MyDrive/StackGAN/Stage_1/Generated_Images/')\n",
        "\n",
        "      # Saving model weights after every 50 epochs\n",
        "      if(i%50==0):\n",
        "        self.generator.save_weights(f'/content/drive/MyDrive/StackGAN/Stage_1/Model_weights/Gen_Epochs:{i+1}.h5')\n",
        "        self.discriminator.save_weights(f'/content/drive/MyDrive/StackGAN/Stage_1/Model_weights/Dis_Epochs:{i+1}.h5')\n",
        "\n",
        "      # Appending losses\n",
        "      self.gen_loss_log.append(gen_epoch_loss)\n",
        "      self.dis_loss_log.append(dis_epoch_loss)\n",
        "\n",
        "      print(\"<><><><><><><><> Time elapsed:\",(time.time()-start),\"<><><><><><><><><><><>\")\n",
        "      print(\"<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\")\n",
        "\n",
        "    with open('/content/drive/MyDrive/StackGAN/Stage_1/Model_weights/Loss.txt', 'w') as file:\n",
        "      # Writing Generator Loss Log file\n",
        "      file.write('Generator Loss:\\n')\n",
        "      for item in self.gen_loss_log:\n",
        "        item=str(item)+'\\n'\n",
        "        file.write(item)\n",
        "\n",
        "      # Writing Discriminator Loss Log file\n",
        "      file.write('Discriminator Loss:\\n')\n",
        "      for item in self.dis_loss_log:\n",
        "        item=str(item)+'\\n'\n",
        "        file.write(item)\n",
        "\n",
        "    self.generator.save_weights('/content/drive/MyDrive/StackGAN/Stage_1/Model_weights/Gen_Epochs:All.h5')\n",
        "    self.discriminator.save_weights('/content/drive/MyDrive/StackGAN/Stage_1/Model_weights/Dis_Epochs:All.h5')"
      ],
      "metadata": {
        "id": "_yZ0cXhCRRvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Stage 1 GAN**"
      ],
      "metadata": {
        "id": "R8Oz0_A1jIuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time=time.time()\n",
        "\n",
        "# Getting the train dataset\n",
        "dataset=gd.Dataset_Generator(img_size=(64, 64))\n",
        "train=dataset.get_train_dataset()\n",
        "\n",
        "# Iterator is passed to the model for training\n",
        "train_iterator=iter(train)"
      ],
      "metadata": {
        "id": "G4yB1kP2jSWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stage 1 GAN model\n",
        "Stage_1_GAN=Stage1_GAN(train_iterator)\n",
        "# Training the model\n",
        "Stage_1_GAN.train_GAN()\n",
        "\n",
        "print(\"Total time elapsed in training:\", (time.time()-start_time))"
      ],
      "metadata": {
        "id": "pGmVmHZY1D9S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8318e9b-4c73-4380-c9ec-cbfe987be1f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
            "<><><><><><><><> Started Epoch: 1 <><><><><><><><><><><><><><><><><><><>\n",
            "tf.Tensor([-0.15178269], shape=(1,), dtype=float32)\n",
            "tf.Tensor([0.00095047], shape=(1,), dtype=float32)\n",
            "\n",
            "Discriminator's Loss after epoch number 0 is= tf.Tensor(0.584998, shape=(), dtype=float32)\n",
            "Generator's Loss after epoch number 0 is= 0.6922483444213867\n",
            "<><><><><><><><> Time elapsed: 35.28979682922363 <><><><><><><><><><><>\n",
            "<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
            "Total time elapsed in training: 633.0350229740143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "soP5bJZ9jyak"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}